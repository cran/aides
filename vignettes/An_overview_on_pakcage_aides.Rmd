---
title: "An overview on R-pakcage aides"
output: 
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    number_sections: true
vignette: >
  %\VignetteIndexEntry{An overview on R-pakcage aides}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<table border = "0" width = "100%" align = "left">
  <tr>
    <td>  
```{r echo = FALSE, fig.align = "left", out.width = 100}
knitr::include_graphics("aides_logo.png")
```
    </td>
    <td> 
    <H3>
    Additive Information & Details of Evidence Synthesis 
    </H3>
    </td>
    <td> 
    [![CRAN](https://img.shields.io/badge/CRAN-v1.3.3-blue.svg?logo=r&logoColor=skyblue)](https://CRAN.R-project.org/package=aides)\
    [![Date](https://img.shields.io/badge/Date-Apr.08.2024-blue.svg?Color=skyblue)](https://cran.r-project.org/package=aides/news/news.html)\
    [![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-blue.svg?color=blue&label=Lifecycle&logo=r&logoColor=skyblue)](https://lifecycle.r-lib.org/articles/stages.html#stable)\
    [![Licence](https://img.shields.io/badge/Licence-GPL--3-blue.svg?logo=gnu&logoColor=skyblue)](https://www.gnu.org/licenses/gpl-3.0.en.html)
    </td>
  </tr>
</table>

\

<table border = "0">
  <tr>
    <td> </td>
    <td> </td>
  </tr>
</table>

\

# Prologue {-}

The *aides*, a R package, emerges as a collection of functions designed to provide supplementary information and intricacies in the processes of data synthesis and evidence evaluation. In the realm of evidence-based decision-making, these processes are pivotal, shaping the foundation upon which informed conclusions are drawn. The *aides*, essentially a toolkit for pooled analysis of aggregated data, is crafted to enhance the inclusivity and depth of this decision-making approach.

Developed with core values of flexibility, ease of use, and comprehensibility, *aides* plays a crucial role in simplifying the often complex analysis process. This accessibility extends to both seasoned professionals and the broader public, fostering a more widespread engagement with synthesized evidence. The significance of such engagement cannot be overstated, as it empowers individuals to navigate through the intricacies of data, promoting a better understanding of the evidence at hand.

Moreover, *aides* is committed to staying at the forefront of advances in the methodology of data synthesis and evidence evaluation. This commitment ensures that users have access to some advanced methods, further enhancing the robustness and reliability of their decision-making processes. In the long term, the overarching goal of the aides package is to contribute to knowledge translation, enabling individuals to make decisions based on a comprehensive understanding of the evidence. In essence, *aides* serves as a beacon, guiding users through the complex terrain of data synthesis and evidence evaluation, ultimately facilitating informed and impactful decision-making. 

Users are suggested to use functions in *aides* by calling the library with following syntax:

```{r}
library(aides)
```

```{r setup, echo = FALSE, warning = FALSE, message = FALSE}
library(aides)
library(meta)
```

\

# Focuses & functions

Briefly, *aides* currently consists of three focuses as follows:

- **Disparity:** a newly proposed assumption regarding disparities in sample size analysis.

- **Discordance:** a newly proposed assumption regarding discordance in rank of study size analysis.

- **Sequential analysis:** a method to examine the sufficiency of information size.

\

Users can import their data and do relevant tests or graphics using functions in package *aides*. The present package consists of eight functions listed as follows: 


<table border = "0" width = "100%" align = "left">
  <tr>
    <td> **Disparity test:** </td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td> `PlotDistrSS()` </td>
    <td> [In the section of Disparity test](#disparityTest) [(Step 3) ](#step-3-TestDisparity) </td>
    <td> [![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg?color=grassgreen)](https://lifecycle.r-lib.org/articles/stages.html#stable) </td>
  </tr>
  <tr>
    <td> `TestDisparity()` </td>
    <td> [In the section of Disparity test](#disparityTest) [(Step 4)](#step-4-TestDisparity) </td>
    <td> [![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg?color=grassgreen)](https://lifecycle.r-lib.org/articles/stages.html#stable) </td>
  </tr>
  <tr>
    <td> `PlotDisparity()` </td>
    <td> [In the section of Disparity test](#disparityTest) [(Step 5)](#step-5-TestDisparity) </td>
    <td> [![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg?color=grassgreen)](https://lifecycle.r-lib.org/articles/stages.html#stable) </td>
  </tr>
  
  
  <tr>
    <td> **Discordance test:** </td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td> `TestDiscordance()` </td>
    <td> [In the section of Discordance test](#discordanceTest) [(Step 3)](#step-3-TestDiscordance) </td>
    <td> [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-brightgreen.svg?color=orange)](https://lifecycle.r-lib.org/articles/stages.html#experimental) </td>
  </tr>
  
  
  <tr>
    <td> **Sequential analysis:** </td>
    <td> </td>
    <td> </td>
  </tr>
  <tr>
    <td> `DoSA()` </td>
    <td> [In the section of Sequential analysis](#sequentialAnalysis) [(Step 2)](#step-2-DoSA) </td>
    <td> [![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg?color=grassgreen)](https://lifecycle.r-lib.org/articles/stages.html#stable) </td>
  </tr>
  <tr>
    <td> `DoOSA()` </td>
    <td> [In the section of Sequential analysis](#sequentialAnalysis) [(additional)](#DoOSA) </td>
    <td> [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-brightgreen.svg?color=orange)](https://lifecycle.r-lib.org/articles/stages.html#experimental) </td>
  </tr>
  <tr>
    <td> `PlotOSA()` </td>
    <td> [In the section of Sequential analysis](#sequentialAnalysis) [(additional)](#PlotOSA) </td>
    <td> [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-brightgreen.svg?color=orange)](https://lifecycle.r-lib.org/articles/stages.html#experimental) </td>
  </tr>
  <tr>
    <td> `PlotPower()` </td>
    <td> [In the section of Sequential analysis](#sequentialAnalysis) [(additional)](#PlotPower) </td>
    <td> [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-brightgreen.svg?color=orange)](https://lifecycle.r-lib.org/articles/stages.html#experimental) </td>
  </tr>
  
</table>

\

<table border = "0">
  <tr>
    <td> </td>
    <td> </td>
  </tr>
</table>

\

# Examples

## Disparity test {#disparityTest}

The following descriptions and syntax outline the process for conducting a disparity test. Users begin by assessing the distribution of sample sizes among studies and visually inspecting them to determine the suitable method for the disparity test. Subsequently, two types of disparity tests are performed: outlier-based and variability-based disparity assessments. Due to the nuanced nature of disparity assumptions, it is advisable to evaluate both outlier-based and variability-based disparities.

<a id="step-1-TestDisparity"></a>
**Step 1:** Import data (example of the study by Olkin 1995)

``` {r, eval = FALSE}
library(meta)
data("Olkin1995")
dataOlkin1995 <- Olkin1995
```

\

<a id="step-2-TestDisparity"></a>
**Step 2:** Process data

``` {r, eval = FALSE}
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
```

\

<a id="step-3-TestDisparity"></a>
**Step 3:** Check distribution of study sizes

The statistical analysis in this phase relies on normality tests such as the Shapiro–Wilk test and the Kolmogorov–Smirnov test ([Conover, 1971](#ref-Conover1971); [Royston, 1982](#ref-Royston1982)). While both tests assume normality as the null hypothesis and generate a test statistic based on the sample, they differ in their sensitivity to features of normal distributions. The Shapiro-Wilk test is specifically designed for testing normality, whereas the Kolmogorov-Smirnov test is more general but less powerful, meaning it is less likely to correctly reject the null hypothesis of normality. 

The Shapiro-Wilk test is specifically designed to assess the normality assumption by evaluating whether a given sample of data comes from a normally distributed population ([Royston, 1982](#ref-Royston1982)). It computes a test statistic ($W$) based on the deviation of sample data from a hypothetical normal distribution. The application of the Shapiro–Wilk method for evaluating the normality of sample sizes entails substituting sample values and sample mean with the respective parameters of interest, namely the number of individuals ($i$) and the average study size ($\overline{i}$). Subsequent to this substitution, the test statistic for assessing the normality of sample sizes ($W_k$) can be computed by incorporating the provided information alongside the study size of each individual study ($i_{(j)}$), while accounting for constants ($c_j$) derived from the number of studies and expected values under the null hypothesis of normality.

Similarly, the Kolmogorov–Smirnov test compares the empirical distribution function (EDF) of the sample dataset to the cumulative distribution function (CDF) of a specified theoretical distribution, such as the normal distribution ([Conover, 1971](#ref-Conover1971)). The test statistic ($D$) represents the maximum absolute difference between the EDF and the CDF. Both tests are commonly employed to ascertain the normality of data before proceeding with further statistical analyses. A straightforward approach to assess normality utilizing the Kolmogorov–Smirnov method involves inputting the study sizes of the individual studies into the designated formula. The test statistic ($D_k$) is then computed accordingly.

As both tests are commonly employed methods, users can simply employ the `shapiro.test()` or `ks.test()` functions in R core package *stats* for testing of sample normality.

``` {r, eval = FALSE}
shapiro.test(dataOlkin1995$n)
ks.test(dataOlkin1995$n, "pnorm")
```

<a id = "results-normality-tests"></a>
``` {r result-distrSS, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
data("Olkin1995")
dataOlkin1995 <- Olkin1995
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
shapiro.test(dataOlkin1995$n)
ks.test(dataOlkin1995$n, "pnorm")
```

\

The *aides* package utilizes them for testing distribution before conducting disparity tests. The analysis can be visualized using the `PlotDistrSS()` function (Figure  \@ref(fig:plot-distrSS)). Default of the`PlotDistrSS()` function is based on the Shapiro-Wilk test.

``` {r, eval = FALSE}
PlotDistrSS(n = n,
            data = dataOlkin1995, 
            study = author, 
            time = year)
```

\

Additionally, if users would like to check normality using Kolmogorov-Smirnov test, they can set parameter `method` with argument `"ks"` in the function `PlotDistrSS()`.

``` {r, eval = FALSE}
PlotDistrSS(n = n,
            data = dataOlkin1995, 
            study = author, 
            time = year,
            method = "ks")
```

``` {r plot-distrSS, fig.cap = "An example for visualization of distribution of study sizes", eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 5, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Olkin1995")
dataOlkin1995 <- Olkin1995
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
PlotDistrSS(n = n,
            data = dataOlkin1995, 
            study = author, 
            time = year,
            method = "ks")
```

\

<a id="step-4-TestDisparity"></a>
**Step 4:** Test assumption of disparity in study size

Essentially, the assessment of outlier-based disparity can be conducted using the function `TestDisparity()`, which requires parameters such as `n`, `data`, `study`, and `time`. The syntax for implementing this analysis is as follows:

``` {r, eval = FALSE}
TestDisparity(n = n, 
              data = dataOlkin1995, 
              study = author, 
              time = year)
```

``` {r result-disparity, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
data("Olkin1995")
dataOlkin1995 <- Olkin1995
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
TestDisparity(n = n, 
              data = dataOlkin1995, 
              study = author, 
              time = year)
```

\

Figure \@ref(fig:plot-disparity-outlier) visualizes the outlier-based disparity based on the excessive cases of oulier(s).

``` {r, eval = FALSE}
TestDisparity(n = n,
              data = dataOlkin1995, 
              study = author, 
              time = year, 
              plot = TRUE)
```

``` {r plot-disparity-outlier, eval = TRUE, fig.cap = "An example for disparity-outlier plot", echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 4, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Olkin1995")
dataOlkin1995   <- Olkin1995
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
TestDisparity(n = n,
              data = dataOlkin1995, 
              study = author, 
              time = year, 
              plot = TRUE)
```

\

<a id="step-5-TestDisparity"></a>
**Step 5:** Illustrate disparity plot

Due to non-normal distribution among the study sizes as shown in Step 3 (Figure \@ref(fig:plot-distrSS) and also see [the results of normality tests](#results-normality-tests)), robust method is recommended for testing variability, which can be carried out by the following syntax:

``` {r, eval = FALSE}
rsltDisparity <- TestDisparity(n = n, 
                               data = dataOlkin1995, 
                               study = author, 
                               time = year,
                               vrblty = "MAD")
```

``` {r result-disparity-vrblty-MAD, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
data("Olkin1995")
dataOlkin1995 <- Olkin1995
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
rsltDisparity <- TestDisparity(n = n, 
                               data = dataOlkin1995, 
                               study = author, 
                               time = year,
                               vrblty = "MAD")
```

\

The following syntax instead of step 5 aforementioned is recommended for illustrating disparity plot of variability based on robust coefficient of variation:

``` {r, eval = FALSE}
PlotDisparity(rsltDisparity, 
              which = "CV", 
              szFntAxsX = 1)
```

``` {r plot-disparity-variability-MAD, eval = TRUE, fig.cap = "An example for disparity-variability (robust) plot", echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 6, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Olkin1995")
dataOlkin1995   <- Olkin1995
dataOlkin1995$n <- dataOlkin1995$n.exp + dataOlkin1995$n.cont
rsltDisparity <- TestDisparity(n = n,
                               data = dataOlkin1995, 
                               study = author, 
                               time = year,
                               vrblty = "MAD", 
                               plot = FALSE)

PlotDisparity(rsltDisparity, 
              which = "CV",
              szFntAxsX = 1)
```

\

## Discordance {#discordanceTest}

The following steps and syntax demonstrate how user can carry out discordance test. Figure \@ref(fig:plot-discordance) visualizes the test.

\

<a id="step-1-TestDiscordance"></a>
**Step 1:** Import data (example of the study by Fleiss 1993)

``` {r, eval = FALSE}
library(meta)
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
```

\

<a id="step-2-TestDiscordance"></a>
**Step 2:** Process data

``` {r, eval = FALSE}
dataFleiss1993bin$n  <- dataFleiss1993bin$n.asp + dataFleiss1993bin$n.plac
dataFleiss1993bin$se <- sqrt((1 / dataFleiss1993bin$d.asp) - (1 / dataFleiss1993bin$n.asp) + (1 / dataFleiss1993bin$d.plac) - (1 / dataFleiss1993bin$n.plac))
```

\

<a id="step-3-TestDiscordance"></a>
**Step 3:** Test assumption of discordance in study size

``` {r, eval = FALSE}
TestDiscordance(n = n, 
                se = se, 
                study = study,
                data = dataFleiss1993bin)
```

``` {r result-discordance, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
data("Fleiss1993bin")
dataFleiss1993bin    <- Fleiss1993bin
dataFleiss1993bin$n  <- dataFleiss1993bin$n.asp + dataFleiss1993bin$n.plac
dataFleiss1993bin$se <- sqrt((1 / dataFleiss1993bin$d.asp) - (1 / dataFleiss1993bin$n.asp) + (1 / dataFleiss1993bin$d.plac) - (1 / dataFleiss1993bin$n.plac))
TestDiscordance(n = n, 
                se = se, 
                study = study,
                data = dataFleiss1993bin)
```

\

<a id="step-4-TestDiscordance"></a>
**Step 4:** Illustrate discordance plot

``` {r, eval = FALSE}
TestDiscordance(n = n, 
                se = se, 
                study = study, 
                data = dataFleiss1993bin, 
                plot = TRUE)
```

``` {r plot-discordance, fig.cap = "An example for discordance plot", eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 6, fig.width = 12, fig.align = "center", out.width = "100%"}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
dataFleiss1993bin$n  <- dataFleiss1993bin$n.asp + dataFleiss1993bin$n.plac
dataFleiss1993bin$se <- sqrt((1 / dataFleiss1993bin$d.asp) - (1 / dataFleiss1993bin$n.asp) + (1 / dataFleiss1993bin$d.plac) - (1 / dataFleiss1993bin$n.plac))
TestDiscordance(n = n, 
                se = se, 
                study = study, 
                data = dataFleiss1993bin, 
                plot = TRUE)
```

\

## Sequential analysis {#sequentialAnalysis}
Sequential analysis in evidence-based medicine is a statistical approach applied in clinical trials and meta-analyses to continually assess accumulating data, allowing for interim decisions on the effectiveness or safety of medical interventions ([Kang, 2021](#ref-Kang2021); [Jennison & Turnbull, 2005](#ref-Jennison2005); [Wetterslev et al., 2017](#ref-Wetterslev2017); [Wetterslev et al., 2008](#ref-Wetterslev2008)). In contrast to traditional methods that wait for all data to be collected, sequential analysis enables periodic assessments during a trial. This method is especially valuable when ongoing assessment is ethically or practically necessary. It seeks to balance the need for robust statistical inference with the ethical duty to ensure participant safety and prompt decision-making that can influence clinical practice. Traditional sequential analysis is proposed for prospective design in terms of trial, and could be adapted for prospective meta-analysis (PMA). Proper planning and pre-specification in study protocols are crucial for maintaining the integrity of sequential analysis ([Thomas et al., 2019](#ref-Thomas2019)). In other words, it is generally discouraged to employ sequential analysis methodologies for conventional meta-analysis (CMA) lacking pre-established parameters within the study protocol. However, it is imperative to acknowledge the persistent relevance of sequential methodologies in facilitating the assessment of error control mechanisms by researchers and clinicians. In scenarios where relevant parameters for sequential analysis remain unspecified, conducting observed sequential analysis serves as a valuable strategy for evaluating the efficacy of error control measures within a synthesis.

### Fundamental method of sequential analysis {#DoSA}
Sequential method has the capability to manage the overall Type I error in clinical trials and cumulative meta-analyses through the utilization of an alpha spending function, exemplified by the approach introduced by [O'Brien & Fleming (1979)](#ref-O'Brien1979). Based on required sample size (*RSS*) or required information size (*RIS*), sequential analysis applies the alpha spending function to allocate less significance level to early interim analyses, demanding more substantial evidence for declaring statistical significance. Critical boundaries for significance are determined by this function, becoming less stringent as more data accumulates. Sequential analysis uses these boundaries to monitor effects at each interim analysis, declaring statistical significance if the cumulative *Z*-score crosses them. This approach minimizes the risk of false positives in sequential analyses, where multiple interim analyses are conducted, while maintaining the ability to detect true effects. 

Therefor, the essentials of sequential meta-analysis encompass various elements including study time, sample size, and cumulative *Z*-scores derived from cumulative meta-analysis following a temporal order. Additionally, integral to this analysis are the anticipated effects of intervention or exposure, which are either predetermined or established through prior estimates, along with their respective variances. Moreover, the formulation accounts for predefined probabilities associated with Type I error (denoted as $\alpha$) and Type II error (denoted as $\beta$). Leveraging this comprehensive set of parameters, particularly the anticipated effects and their associated variances in conjunction with $\alpha$ and $\beta$, this methodology facilitates the computation of the *RIS* and the establishment of alpha-spending monitoring boundaries. By charting the information size axis, the analysis offers a comprehensive visualization of the cumulative *Z*-scores and the accompanying alpha-spending monitoring boundaries ([Wetterslev et al., 2008](#ref-Wetterslev2008); [Wetterslev et al., 2009](#ref-Wetterslev2009); [Wetterslev et al., 2017](#ref-Wetterslev2017)).

Within the realm of sequential method, the concept of the *RIS* holds paramount significance as it serves as the cornerstone for establishing spending monitoring boundaries, necessitated by adjustments to accommodate multiple interim analyses ([Wetterslev et al., 2017](#ref-Wetterslev2017)). Essentially, sequential methods encompass considerations pertinent to sample size determination, albeit employing an approach divergent from conventional methodologies employed in hypothesis testing or estimation. The determination of the information size, delineating the *RSS*, hinges crucially upon various parameters including the anticipated effect size, statistical power, significance level, and any requisite adjustments tailored for multiple testing scenarios. Conventionally, the determination of the *RSS* encompasses an assessment of intervention or exposure effects, their variance, as well as the predefined probabilities associated with Type I error ($\alpha$) and Type II error ($\beta$). The formula governing the calculation of the *RSS* is as follows:

\begin{equation}
RSS = 4 \times (Z_{1-\alpha/2} + Z_{1 - \beta})^2 \times \frac{\sigma^2} {\delta^2}
(\#eq:eq-RRS)
\end{equation}

Where 
:   $\alpha$ represents the predefined overall probabilities of false positive. \
    $\beta$ represents the predefined overall probabilities of false negative. \
    $\delta$ refers to the assumed effect, representing either the minimal or expected meaningful effect. \
    $\sigma^2$ refers to the variance that associated with the assumed effect.
    
\

In the context of dichotomous outcomes, the parameter $\delta$ can be substituted with the effect size that a trial aims to address, denoted by $\mu$, derived from the difference in proportions between the control and experimental groups, represented by $P_{control} - P_{experiment}$. Here, $P_{control}$ and $P_{experiment}$ signify the proportions of observed events in the control and experimental groups, respectively. The variance $\sigma^2$ is computed as $P_{average} \times (1 - P_{average})$, where $P_{average}$ is derived from $(P_{control} + P_{experiment}) / 2$ ([Wetterslev et al., 2017](#ref-Wetterslev2017)). For continuous outcomes, the *RSS* can be determined as follows:

\begin{equation}
RSS_{continuous} = 4 \times (Z_{1-\alpha/2} + Z_{1 - \beta})^2 \times \frac{MD^2} {SD^2}
(\#eq:eq-RRS-continuous)
\end{equation}

Where 
:   $\alpha$ represents the predefined overall probabilities of false positive. \
    $\beta$ represents the predefined overall probabilities of false negative. \
    $MD$ refers to the assumed mean difference between two groups. \
    $SD$ refers to the standard deviation that associated with the assumed mean difference. 
    
\

Based on the basic formula for calculating *RSS*, *RIS* in sequential method could be calculated by using assumed pooled effect and the variance that associated with the assumed pooled effect ([Wetterslev et al., 2017](#ref-Wetterslev2017)). Thus, the formula for the calculation of the *RIS* for meta-analysis in fixed-effect model is as follows:

\begin{equation}
RIS_{fixed} = 4 \times (Z_{1-\alpha/2} + Z_{1 - \beta})^2 \times \frac{V_{fixed}} {\theta^2}
(\#eq:eq-RIS-fixed)
\end{equation}

Where 
:   $\alpha$ represents the predefined overall probabilities of false positive. \
    $\beta$ represents the predefined overall probabilities of false negative. \
    $\theta$ refers to the assumed pooled effect, representing either the minimal or expected meaningful effect. \
    $V_{fixed}$ refers to the variance that associated with the assumed pooled effect in fixed-effect model. 
    
\

In meta-analysis employing the random-effects model, the calculation of the *RIS* follows a similar formula, but with a substitution of the variance associated with the assumed pooled effect by the variance specific to the random-effects model. *RIS* for meta-sequential analysis in random-effects model can be denoted as $RIS_{random}$. For a conservative estimation of the *RIS* in meta-analyses featuring heterogeneity, it is advisable to incorporate a heterogeneity correction factor ([Wetterslev et al., 2008](#ref-Wetterslev2008)), also referred to as the Adjusted Factor (*AF*) in Trial Sequential Analysis (TSA) ([Wetterslev et al., 2009](#ref-Wetterslev2009); [Wetterslev et al., 2017](#ref-Wetterslev2017)). Obviously, the calculation of the heterogeneity correction factor is inherently tied to the heterogeneity statistics. The I-squared statistic (denoted as $I^2$) is particularly suited for this purpose, serving as a widely utilized measure for quantifying and assessing heterogeneity ([Higgins & Thompson, 2002](#ref-Higgins2002)). $I^2$-based *AF* (denoted as $AF_{I^2}$) can be derived from the formula as follow:

\begin{equation}
AF_{I^2} = \frac{1} {(1 - I^2)}
(\#eq:eq-AF-I2)
\end{equation}

\

However, $I^2$ may provide misleading estimates due to uncertainties in the sampling error could introduce bias or inaccuracies into the analysis. Alternative measures or adjustments that are less susceptible to issues related to sampling error estimation should be considered ([Wetterslev et al., 2009](#ref-Wetterslev2009)). Diversity (denoted as $D^2$) has been proposed as a pivotal parameter in the computation of *AF* for the *RIS* in random-effects meta-analysis ([Wetterslev et al., 2009](#ref-Wetterslev2009); [Wetterslev et al., 2017](#ref-Wetterslev2017)). It is derived from the variances observed in both the random-effects model ($V_{random}$) and the fixed-effect model ($V_{fixed}$). The formula for calculating $D^2$ is as follows:

\begin{equation}
D^2 = \frac{(V_{random} - V_{fixed})} {V_{random}}
(\#eq:eq-D2)
\end{equation}

\

Then, the calculation of $D^2$-based *AF* (denoted as $AF_{D^2}$) is similar to the formula of $AF_{I^2}$ with replacement of $I^2$ by $D^2$. When addressing concerns regarding sampling error or bias, $AF_{D^2}$ offers notable advantages over $AF_{I^2}$ because $D^2$ operates without the prerequisite of assuming a typical sampling error (usually denoted as $\sigma^2$). Besides, it accurately mirrors the relative variance expansion resultant from the between-trial variance estimate, and exhibits a propensity to diminish as the estimate decreases, even when applied to identical study sets ([Wetterslev et al., 2009](#ref-Wetterslev2009)).

To provide a conservative estimation of the *RIS* for meta-analyses with heterogeneity, it is recommended to utilize an adjusted RIS. This can be achieved by multiplying $RIS_{random}$ by the *AF* ([Wetterslev et al., 2009](#ref-Wetterslev2009)). Subsequently, there are two the I-squared-adjusted information size (*HIS*) can be derived by further multiplying $RIS_{random}$ by $AF_{I^2}$. Similarly, the diversity-adjusted information size (DIS) is obtained by calculating $RIS_{random}$ multiplied by $AF_{D^2}$, as suggested by [Wetterslev et al. (2009)](#ref-Wetterslev2009).

The following steps and syntax demonstrate how user can carry out sequential analysis. Figure \@ref(fig:plot-sequential) sequential analysis plot. This demonstration serves as a guide for users intending to conduct a prospective meta-analysis with predefined parameters such as Type I error ($\alpha$), Type II error ($\beta$), presumed effect size (relative risk reduction, denoted as $RRR$, for dichotomous outcomes; mean difference, denoted as $MD$, for continuous outcomes), an estimate of the variability associated with the presumed effect. However, this demonstration may not be applicable for meta-analyses conducted without such pre-specified parameters.

\

<a id="step-1-DoSA"></a>
**Step 1:** Import data (example of the study by Fleiss 1993)

``` {r, eval = FALSE}
library(meta)
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
```

\

<a id="step-2-DoSA"></a>
**Step 2:** Do sequential analysis

``` {r, eval = FALSE}
DoSA(Fleiss1993bin, 
     source = study, 
     time = year,
     r1 = d.asp, 
     n1 = n.asp, 
     r2 = d.plac, 
     n2 = n.plac, 
     measure = "RR",
     PES = 0.1,
     RRR = 0.2,
     group = c("Aspirin", "Placebo"))
```

``` {r result-DoSA, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
DoSA(Fleiss1993bin, 
     source = study, 
     time = year,
     r1 = d.asp, 
     n1 = n.asp, 
     r2 = d.plac, 
     n2 = n.plac, 
     measure = "RR",
     PES = 0.1,
     RRR = 0.2,
     group = c("Aspirin", "Placebo"))
```

\

<a id="step-3-DoSA"></a>
**Step 3:** Visualize sequential analysis

``` {r, eval = FALSE}
DoSA(Fleiss1993bin, 
     source = study, 
     time = year,
     r1 = d.asp, 
     n1 = n.asp, 
     r2 = d.plac, 
     n2 = n.plac, 
     measure = "RR",
     PES = 0.1,
     RRR = 0.2,
     group = c("Aspirin", "Placebo"),
     plot = TRUE)
```

``` {r plot-sequential, fig.cap = "An example for sequential analysis", eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 6, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
DoSA(Fleiss1993bin, 
     source = study, 
     time = year,
     r1 = d.asp, 
     n1 = n.asp, 
     r2 = d.plac, 
     n2 = n.plac, 
     measure = "RR",
     PES = 0.1,
     RRR = 0.2,
     group = c("Aspirin", "Placebo"),
     plot = TRUE)
```

\

### Observed sequential analysis {#DoOSA}
Observed sequential analysis is recommended for those pooled analysis without pre-specified parameters for sequential analysis. In this situation, thus, Step 2 should use following syntax:

``` {r, eval = FALSE}
DoOSA(Fleiss1993bin, 
      source = study, 
      time = year,
      r1 = d.asp, 
      n1 = n.asp, 
      r2 = d.plac, 
      n2 = n.plac, 
      measure = "RR",
      group = c("Aspirin", "Placebo"))
```

``` {r result-OSA, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
DoOSA(Fleiss1993bin, 
      source = study, 
      time = year,
      r1 = d.asp, 
      n1 = n.asp, 
      r2 = d.plac, 
      n2 = n.plac, 
      measure = "RR",
      group = c("Aspirin", "Placebo"))
```

\

#### Visualize basic plot of observed sequential analysis (example of the study by Fleiss 1993) {#PlotOSA}

Observed sequential analysis could be visualized using the same function in terms of `DoOSA()` with argument `TRUE` for the parameter `plot`. The syntax and graph are shown as follows.

``` {r, eval = FALSE}
DoOSA(Fleiss1993bin,
      source = study,
      time = year,
      r1 = d.asp,
      n1 = n.asp,
      r2 = d.plac,
      n2 = n.plac,
      measure = "RR",
      group = c("Aspirin", "Placebo"),
      plot = TRUE)
```

``` {r plot-OSA, fig.cap = "An example for illustrating observed sequential analysis", eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 6, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
DoOSA(Fleiss1993bin,
      source = study,
      time = year,
      r1 = d.asp,
      n1 = n.asp,
      r2 = d.plac,
      n2 = n.plac,
      measure = "RR",
      group = c("Aspirin", "Placebo"),
      plot = TRUE)
```

\

#### Observed sequential analysis with colorful zones (example of the study by Fleiss 1993) {#PlotOSAZone}

If users would like to show colorful zones of observed sequential analysis, two steps are recommended. Firstly, they need to store the output of function `DoOSA()` with argument `TRUE` for parameter `Plot`, then colorfur zones on the observed sequential analysis can be carried out using function `PlotOSA()` with argument `TRUE` for parameter `lgcZone`.

\

<a id="step-1-PlotOSA-zone"></a>
**Step 1:** Store observed sequential analysis with argument `TRUE` for parameter `Plot`

``` {r, eval = FALSE}
output <- DoOSA(Fleiss1993bin,
                source = study,
                time = year,
                r1 = d.asp,
                n1 = n.asp,
                r2 = d.plac,
                n2 = n.plac,
                measure = "RR",
                group = c("Aspirin", "Placebo"),
                SAP = TRUE)
```

\

<a id="step-2-PlotOSA-zone"></a>
**Step 2:** Show colorful zone on the observed sequential analysis with argument `TRUE` for parameter `lgcZone`

``` {r, eval = FALSE}
PlotOSA(output,
        lgcZone = TRUE)
```

``` {r plot-OSA-zone, fig.cap = "An example for illustrating colorful zones on observed sequential analysis", eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 8, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
output <- DoOSA(Fleiss1993bin, 
                source = study,
                time = year,
                r1 = d.asp, 
                n1 = n.asp, 
                r2 = d.plac, 
                n2 = n.plac, 
                measure = "RR",
                group = c("Aspirin", "Placebo"),
                plot = FALSE,
                SAP = TRUE)

PlotOSA(output,
        lgcZone = TRUE)
```

\

#### Power curve of observed sequential analysis {#PlotPower}

If users would like to further illustrate power curve of the observed sequential analysis, two steps are recommended. Firstly, they need to store the output of function `DoOSA()` with argument `TRUE` for parameter `SAP`, then plot power curve of the observed sequential analysis using function `PlotPower()`.

\

<a id="step-1-OSA-Power-curve"></a>
**Step 1:** Store output of observed sequential analysis with argument `TRUE` for parameter `SAP`

``` {r, eval = FALSE}
output <- DoOSA(Fleiss1993bin,
                source = study,
                time = year,
                r1 = d.asp,
                n1 = n.asp,
                r2 = d.plac,
                n2 = n.plac,
                measure = "RR",
                group = c("Aspirin", "Placebo"),
                SAP = TRUE)
```

\

<a id="step-2-OSA-Power-curve"></a>
**Step 2:** Power curve of observed sequential analysis

``` {r, eval = FALSE}
PlotPower(output)
```

``` {r plot-power, fig.cap = "An example for illustrating plot curve of observed sequential analysis", eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, results = "hide", fig.height = 6, fig.width = 8, fig.align = "center", out.width = "100%"}
data("Fleiss1993bin")
dataFleiss1993bin <- Fleiss1993bin
output <- DoOSA(Fleiss1993bin, 
                source = study,
                time = year,
                r1 = d.asp, 
                n1 = n.asp, 
                r2 = d.plac, 
                n2 = n.plac, 
                measure = "RR",
                group = c("Aspirin", "Placebo"),
                plot = FALSE,
                SAP = TRUE)

PlotPower(output)
```

\

## Reference {-}

<a id="ref-Conover1971"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Conover, W. J. (1971). *Practical Nonparametric Statistics*. New York: John Wiley & Sons. Pages 295–301.
</p>

<a id="ref-Higgins2002"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Higgins, J. P., & Thompson, S. G. (2002). Quantifying heterogeneity in a meta‐analysis. *Statistics in medicine, 21(11)*, 1539-1558. https://doi.org/10.1002/sim.1186
</p>

<a id="ref-Jennison2005"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Jennison, C., & Turnbull, B. W. (2005). Meta-analyses and adaptive group sequential designs in the clinical development process. *Journal of biopharmaceutical statistics, 15(4)*, 537–558. https://doi.org/10.1081/BIP-200062273
</p>

<a id="ref-Kang2021"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Kang, H. (2021). Trial sequential analysis: novel approach for meta-analysis. *Anesthesia and Pain Medicine, 16(2)*, 138-150.  https://doi.org/10.17085/apm.21038
</p>

<a id="ref-O'Brien1979"></a>
<p style="text-indent: -25px; margin-left: 25px;">
O'Brien, P. C., & Fleming, T. R. (1979). A multiple testing procedure for clinical trials. *Biometrics, 35(3)*, 549-556. https://doi.org/10.2307/2530245
</p>

<a id="ref-Royston1982"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Royston, P. (1982). An extension of Shapiro and Wilk's WW test for normality to large samples. *Applied Statistics, 31*, 115–124. doi:10.2307/2347973. https://www.jstor.org/stable/2347973
</p>

<a id="ref-Thomas2019"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Thomas, J., Askie, L. M., Berlin, J. A., Elliott, J. H., Ghersi, D., Simmonds, M., Takwoingi, Y., Tierney, J. F., Higgins, J. P. T. (2019). Prospective approaches to accumulating evidence. In Higgins J. P. T., & Green, S., (Eds.), *Cochrane Handbook for Systematic Reviews of Interventions*. Chichester (UK): John Wiley & Sons. https://training.cochrane.org/handbook/archive/v6/chapter-22
</p>

<a id="ref-Wetterslev2008"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Wetterslev, J., Thorlund, K., Brok, J., & Gluud, C. (2008). Trial sequential analysis may establish when firm evidence is reached in cumulative meta-analysis. *Journal of clinical epidemiology, 61(1)*, 64-75. https://doi.org/10.1016/j.jclinepi.2007.03.013
</p>

<a id="ref-Wetterslev2009"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Wetterslev, J., Thorlund, K., Brok, J., & Gluud, C. (2009). Estimating required information size by quantifying diversity in random-effects model meta-analyses. *BMC medical research methodology, 9(1)*, 1-12. https://doi.org/10.1186/1471-2288-9-86
</p>

<a id="ref-Wetterslev2017"></a>
<p style="text-indent: -25px; margin-left: 25px;">
Wetterslev, J., Jakobsen, J. C., & Gluud, C. (2017). Trial sequential analysis in systematic reviews with meta-analysis. *BMC medical research methodology, 17(1)*, 1-18. https://doi.org/10.1186/s12874-017-0315-7
</p>
